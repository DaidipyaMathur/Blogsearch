{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b4ea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daidipya-mathur/bsearch/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 4735.37 examples/s]\n",
      "Casting the dataset: 100%|██████████| 50/50 [00:00<00:00, 6055.88 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset prepared for training ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['url', 'title', 'text', 'classification_v1', 'human_classifier', 'label'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['url', 'title', 'text', 'classification_v1', 'human_classifier', 'label'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "\n",
      "Example from the training set:\n",
      "{'url': 'http://store.waitbutwhy.com/collections/plush-toys', 'title': 'Plush Toys - Wait But Why Store', 'text': 'Plush Toys - Wait But Why Store Menu 0 Store Blog home about archive minis the shed dinner table support wbw Posters All Posters Life Calendar Wrapping Paper One in a Million Litographs A Perspective on Time Dark Playground Life Mountain Apparel Unisex Women Kid\\'s Sweatshirts All Sweatshirts Crewnecks Hoodies Characters Instant Gratification Monkey The Panic Monster The Mammoth Mugs Accessories Plushies Cards & Wrapping Paper Tote Bags Buttons Coffee Mugs Stickers Login USD CAD AUD GBP EUR JPY 0 Your Cart is Empty Continue Shopping $ 0.00 Subtotal USD CAD AUD GBP EUR JPY Login Store Blog Posters Apparel Characters Mugs Accessories home about archive minis the shed dinner table support wbw All Posters Life Calendar Wrapping Paper One in a Million Litographs A Perspective on Time Dark Playground Life Mountain Unisex Women Kid\\'s Sweatshirts All Sweatshirts Crewnecks Hoodies Instant Gratification Monkey The Panic Monster The Mammoth Plushies Cards & Wrapping Paper Tote Bags Buttons Coffee Mugs Stickers Men\\'s Tees Tank Tops Sweatshirts Women\\'s Tees Tees Racerback Kid\\'s Tees Toddler Tees Onesies Sweatshirts All Sweatshirts Crewnecks Hoodies Men\\'s Tees Tank Tops Sweatshirts Women\\'s Tees Tees Racerback Kid\\'s Tees Toddler Tees Onesies Sweatshirts All Sweatshirts Crewnecks Hoodies Characters Instant Gratification Monkey The Panic Monster The Mammoth Men\\'s Tees Tank Tops Sweatshirts Women\\'s Tees Tank Tops Racerbacks Sweatshirts Accessories Plushies Cards & Wrapping Paper Tote Bags Buttons Coffee Mugs Stickers Styles Plushies Men Women Kids Posters Accessories Characters Instant Gratification Monkey The Panic Monster The Mammoth Men\\'s Tees Tank Tops Sweatshirts Women\\'s Tees Tank Tops Racerbacks Sweatshirts Accessories Plushies Cards & Wrapping Paper Tote Bags Buttons Coffee Mugs Stickers Styles Plushies Men Women Kids Posters Accessories Home / Plush Toys / Page 1 of 1 Featured Best Selling Alphabetically: A-Z Alphabetically: Z-A Price: Low to High Price: High to Low Date: New to Old Date: Old to New Men\\'s Tees Tank Tops Sweatshirts Women\\'s Tees Tank Tops Racerbacks Sweatshirts Types Buttons Gift Card Greeting Cards Hat Kids/Infants Mug Phone Cases Poster Stickers Stuffed Toys Sweatshirt Tank Top Tote Bag Tshirt Women\\'s Tank Top Women\\'s Tshirt Recently Viewed Recently Viewed Instant Gratification Monkey Plush Toy Quick View Notify me when this product is available: Qty Add to Cart Instant Gratification Monkey Plush Toy $ 35.00 The Mammoth Plush Toy Quick View Notify me when this product is available: Qty Add to Cart The Mammoth Plush Toy $ 35.00 The Panic Monster Plush Toy Quick View Notify me when this product is available: Qty Add to Cart The Panic Monster Plush Toy $ 35.00 WBW Plushie Bundle Pack Quick View Notify me when this product is available: Qty Add to Cart WBW Plushie Bundle Pack $ 90.00 $ 105.00 Wait But Why Homepage About Archive Dinner Table Privacy Policy Styles Plushies Men Women Kids Posters Accessories New Post Every Sometimes © 2025 Wait But Why Store . Sizing Guide Hand screen printed in California Pre-shrunk 100% combed ring-spun fine jersey cotton 30 singles 4.5 ounce super soft t-shirt Measurements are taken of the front of the shirt. \\xa0For example, a Men\\'s Small says Chest = 18\", which means it measures 18\" from left to right of the front of the shirt. \\xa0 The full shirt circumference = 36\" (front and back combined).', 'classification_v1': 'other', 'human_classifier': 'other', 'label': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset, DatasetDict, ClassLabel # <-- Import ClassLabel\n",
    "\n",
    "# --- Load your manually labeled data ---\n",
    "# (This part is the same)\n",
    "ground_truth_file = 'ground_truth.jsonl'\n",
    "data = []\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "full_dataset = Dataset.from_list(data)\n",
    "\n",
    "# --- Create the labels for our model ---\n",
    "# (This part is the same)\n",
    "labels = [\"other\", \"personal_blog\"]\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "def add_labels(examples):\n",
    "    # This assumes you changed your file to have \"human_classification\"\n",
    "    return {'label': [label2id[label] for label in examples['human_classifier']]}\n",
    "\n",
    "full_dataset = full_dataset.map(add_labels, batched=True)\n",
    "\n",
    "# ==========================================================\n",
    "# ---  THE NEW LINE IS HERE  ---\n",
    "# Convert the 'label' column to the ClassLabel type\n",
    "# ==========================================================\n",
    "full_dataset = full_dataset.cast_column(\"label\", ClassLabel(names=labels))\n",
    "\n",
    "\n",
    "# --- Split the data into training and testing sets ---\n",
    "# (This part is the same and will now work)\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
    "\n",
    "# Create a final DatasetDict object which is the standard format\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "\n",
    "print(\"--- Dataset prepared for training ---\")\n",
    "print(dataset_dict)\n",
    "print(\"\\nExample from the training set:\")\n",
    "print(dataset_dict['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c57285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 64.92 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 252.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tokenization Complete ---\n",
      "\n",
      "Our tokenized dataset now has new features:\n",
      "{'url': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'classification_v1': Value(dtype='string', id=None), 'human_classifier': Value(dtype='string', id=None), 'label': ClassLabel(names=['other', 'personal_blog'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "\n",
      "Example of the new features for the first training example:\n",
      "input_ids (first 20): [101, 27729, 10899, 1011, 3524, 2021, 2339, 3573, 12183, 1014, 3573, 9927, 2188, 2055, 8756, 7163, 2015, 1996, 8328, 4596]\n",
      "attention_mask (first 20): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# The name of the pre-trained model we will use\n",
    "model_checkpoint = \"distilbert-base-uncased\" \n",
    "\n",
    "# Load the tokenizer that corresponds to our model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Create a function that will tokenize our text\n",
    "def tokenize_function(examples):\n",
    "    # The tokenizer will pad shorter texts and truncate longer ones to a standard length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenizer to our entire dataset (both train and test splits)\n",
    "# The `batched=True` flag makes this process much faster.\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "print(\"--- Tokenization Complete ---\")\n",
    "print(\"\\nOur tokenized dataset now has new features:\")\n",
    "print(tokenized_datasets['train'].features)\n",
    "\n",
    "print(\"\\nExample of the new features for the first training example:\")\n",
    "# Let's look at the first example again to see the new fields\n",
    "first_example = tokenized_datasets['train'][0]\n",
    "print(\"input_ids (first 20):\", first_example['input_ids'][:20])\n",
    "print(\"attention_mask (first 20):\", first_example['attention_mask'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb7f580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in ./.venv/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch<2.7,>=2.1 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (1.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch<2.7,>=2.1->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch<2.7,>=2.1->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in ./.venv/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.26.0) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.1.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4329/214739532.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Tuning ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 02:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.651261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.610426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install 'accelerate>=0.26.0'\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# --- Load the Pre-trained Model ---\n",
    "# We load the DistilBERT model and specify it's for sequence classification.\n",
    "# We also tell it how many labels we have (2: \"other\" and \"personal_blog\")\n",
    "# and provide our label2id/id2label mappings.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# --- Define Training Arguments ---\n",
    "# This object contains all the settings for the training run.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"blog_classifier_model\",  # The directory where the final model will be saved\n",
    "    learning_rate=2e-5,                  # A standard, good learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,       # How many examples to process at once during training\n",
    "    per_device_eval_batch_size=8,        # How many examples to process at once during evaluation\n",
    "    num_train_epochs=3,                  # The number of times to go through the entire training dataset\n",
    "    weight_decay=0.01,                   # A technique to prevent the model from overfitting\n",
    "    eval_strategy=\"epoch\",         # Evaluate performance at the end of each epoch\n",
    "    save_strategy=\"epoch\",               # Save a checkpoint of the model at the end of each epoch\n",
    "    load_best_model_at_end=True,         # Automatically load the best performing model at the end\n",
    ")\n",
    "\n",
    "# --- Create the Trainer ---\n",
    "# The Trainer brings together the model, arguments, datasets, tokenizer, and evaluation metrics.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# --- Start Training! ---\n",
    "print(\"--- Starting Fine-Tuning ---\")\n",
    "trainer.train()\n",
    "print(\"--- Fine-Tuning Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
